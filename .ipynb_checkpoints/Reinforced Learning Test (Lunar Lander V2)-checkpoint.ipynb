{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aca5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d5f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686faa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEP Q-LEARNING NETWORK CLASS\n",
    "\n",
    "# DQN is a derived class of the nn.Module class\n",
    "class DQN(nn.Module):\n",
    "    \n",
    "    # derived class constructor\n",
    "    def __init__(self, img_height, img_width): \n",
    "        \n",
    "        # base class constructor\n",
    "        super().__init__()\n",
    "\n",
    "        # define the layers of weights and biases\n",
    "        # nn.Linear means fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=img_height*img_width*3, out_features=36)   \n",
    "        self.fc2 = nn.Linear(in_features=36, out_features=48)\n",
    "        self.out = nn.Linear(in_features=48, out_features=4)\n",
    "        \n",
    "    # feed forward method\n",
    "    # t is a tensor, an image is a tensor of rank 2\n",
    "    def forward(self, t):\n",
    "        \n",
    "        # reduce the tensor rank from 2 to 1\n",
    "        t = t.flatten(start_dim=1)\n",
    "        \n",
    "        # use ReLu from torch.nn.functional as activation function for layer 1 and layer 2\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        \n",
    "        # output layer without activation function\n",
    "        t = self.out(t)\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283866fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXPERIENCE CLASS\n",
    "\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'next_state', 'reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLAY MEMORY CLASS\n",
    "\n",
    "class ReplayMemory():\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "    \n",
    "    # method to record a new experience\n",
    "    def push(self, experience):\n",
    "        \n",
    "        # append experience to memory if memory is not full\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "            \n",
    "        # cyclically overwrite old memories if memory if full\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "            \n",
    "        # push count used to keep track of where the cycle is\n",
    "        self.push_count += 1\n",
    "        \n",
    "    # method to sample a random batch of experience from the experience pool\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    # method to check if replay memory is big enough to satisfy the requested sample size\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2175142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPSILON GREEDY STRATEGY CLASS\n",
    "\n",
    "class EpsilonGreedyStrategy():\n",
    "    \n",
    "    def __init__(self, start, end, decay):\n",
    "        \n",
    "        # starting epsilon value. range from 0 to 1\n",
    "        self.start = start\n",
    "        \n",
    "        # ending epsilon value, should be smaller than starting epsilon value\n",
    "        self.end = end\n",
    "        \n",
    "        self.decay = decay\n",
    "        \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENT CLASS\n",
    "\n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # strategy = EpsilonGreedyStrategy()\n",
    "        self.strategy = strategy\n",
    "        \n",
    "        # num_actions is the total number of actions that the agent can make\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # computing device\n",
    "        self.device = device\n",
    "    \n",
    "    # policy_net is an instance of class DQN\n",
    "    # state is a representation of the input image\n",
    "    def select_action(self, state, policy_net):\n",
    "        \n",
    "        # get exploration rate according to current step\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # explore\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            \n",
    "            # pass action as a tensor to the device (GPU)\n",
    "            return torch.tensor([action]).to(self.device)\n",
    "        \n",
    "        # exploit\n",
    "        else:\n",
    "            \n",
    "            # turn off gradient since there is no training involved in an inference\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # argmax(dim=1) returns the index of the highest value in the input tensor, and reduces the rank by 1\n",
    "                # policy_net(state) calls the forward method in the DQN class, as a built in feature\n",
    "                return policy_net(state).argmax(dim=1).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ec343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENVIRONMENT CLASS\n",
    "\n",
    "class LunarLanderEnvManager():\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        \n",
    "        # unwrapped allows access to behind-the-scene dynamics\n",
    "        self.env = gym.make('LunarLander-v2').unwrapped\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "        \n",
    "    # reset encapsulation\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "    \n",
    "    # close encapsulation\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "    \n",
    "    # render encapsulation\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "    \n",
    "    # get total number of available actions (none, left, main, right)\n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space.n\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        \n",
    "        # underscore ignores values\n",
    "        # item() returns value as standard python number\n",
    "        _, reward, self.done, _ = self.env.step(action.item())\n",
    "        \n",
    "        # pass reward as a tensor to the device (GPU)\n",
    "        return torch.tensor([reward], device=self.device)\n",
    "    \n",
    "    # check if the game is at initial state\n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "    \n",
    "    def get_state(self):\n",
    "    \n",
    "        # return black screen if game is just starting or has ended\n",
    "        if self.just_starting() or self.done:\n",
    "            \n",
    "            # save current screen for later use, but return black screen instead\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        \n",
    "        # return the difference between the previous screen and the current screen\n",
    "        # in order to account for the change between time steps\n",
    "        else:\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2\n",
    "            return s2 - s1\n",
    "        \n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "    \n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "    \n",
    "    def get_processed_screen(self):\n",
    "    \n",
    "        # render returns a 3D matrix of dimensions height, width, channel (R, G, B)\n",
    "        # transpose changes the matrix axes to channel, height, width\n",
    "        screen = self.render('rgb_array').transpose((2, 0, 1))\n",
    "        \n",
    "        return self.transform_screen_data(screen)\n",
    "    \n",
    "    # convert screen data into tensor\n",
    "    def transform_screen_data(self, screen):\n",
    "        \n",
    "        # convert screen data into a contiguous float data array in the memory\n",
    "        # map the value from 0-255 to 0-1\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        \n",
    "        # convert screen data into tensor data\n",
    "        screen = torch.from_numpy(screen)\n",
    "        \n",
    "        # use torchvision package to compose image transforms\n",
    "        # torchvision.transform as T\n",
    "        resize = T.Compose([\n",
    "            \n",
    "            # transform into PIL image to resize\n",
    "            # PIL: Python Image Library\n",
    "            T.ToPILImage(),\n",
    "            T.Resize([100,150]),\n",
    "            \n",
    "            # transform back to tensor after resizing\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        # resize screen data\n",
    "        # unsqueeze adds an extra dimension to the tensor, for tensors are passed in batches\n",
    "        return resize(screen).unsqueeze(0).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NON-PROCESSED SCREEN EXAMPLE\n",
    "\n",
    "# device = torch.device(\"cuda\" )\n",
    "\n",
    "# # environment manager\n",
    "# em = LunarLanderEnvManager(device)\n",
    "# em.reset\n",
    "# screen = em.render('rgb_array')\n",
    "# em.close()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(screen)\n",
    "# plt.title('Non-processed screen example')\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cedf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PROCESSED SCREEN EXAMPLE\n",
    "\n",
    "# screen = em.get_processed_screen()\n",
    "# em.close()\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "# # move screen data back to cpu to use numpy functions like squeeze\n",
    "# # squeeze removes dimensions with length 1\n",
    "# # use permute to move channel dimension back to 3rd dimension\n",
    "# plt.imshow(screen.cpu().squeeze(0).permute(1, 2, 0), interpolation='none')\n",
    "\n",
    "# plt.title('Processed screen example')\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b61da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PERFORMANCE PLOTS\n",
    "\n",
    "# values is the time taken for each episode\n",
    "# moving average is a statistical average that takes average over a certain period\n",
    "def plot(episode_rewards, moving_avg_period, current_step, eps_decay, episode_losses):\n",
    "    \n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(episode_rewards)\n",
    "    moving_avg = get_moving_average(moving_avg_period, episode_rewards)\n",
    "    plt.plot(moving_avg)\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Average Loss')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.plot(episode_losses)\n",
    "    plt.pause(0.001)\n",
    "    \n",
    "    print(\"Episode\", len(episode_rewards), \"\\t\", moving_avg_period, \"Episodes average reward:\", moving_avg[-1])\n",
    "    print(\"Steps\", current_step, \\\n",
    "        \"\\tExploring probability\", math.exp(-1. * current_step * eps_decay))\n",
    "    print(\"Average loss:\\t\", np.mean(episode_losses))\n",
    "    if is_ipython: display.clear_output(wait=True)\n",
    "    \n",
    "def get_moving_average(period, values):\n",
    "    \n",
    "    # convert to float tensor\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    \n",
    "    if len(values) >= period:\n",
    "        \n",
    "        # check documentation for unfold\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1).mean(dim=1).flatten(start_dim=0)\n",
    "        \n",
    "        # concatenate zeros to the front of the moving averages\n",
    "        # since the first period-1 values cannot provide a moving average\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "        \n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ff219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSOR PROCESSING FUNCTION\n",
    "\n",
    "def extract_tensors(expriences):\n",
    "    \n",
    "    # zip takes in a set of array iterators (eg. experiences[0], experiences[1], ...) and\n",
    "    # returns an array containing the 0th element of each array, another array containing the 1st element of each array,\n",
    "    # and another array containing... etc.\n",
    "    # the * operator unpacks the list into arguments\n",
    "    batch = Experience(*zip(*experiences))\n",
    "    \n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "    \n",
    "    return (t1, t2, t3, t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-VALUE CALCULATOR CLASS\n",
    "\n",
    "class QValues():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        \n",
    "        # gather uses actions to select the corresponding state value in the 'states' tensor\n",
    "        # -1 in pytorch lets the program figure out what dimensions to unsqueeze\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_next(target_next, next_states):\n",
    "        \n",
    "        # from the next_states tensor, flatten the height dimension (0th dimension is for different next states),\n",
    "        # then find the max value along the flattened dimension (2nd dimension is now 1st dimension),\n",
    "        # [0] just takes the value inside the tensor\n",
    "        # if the value is equal to 0, that state is a black screen state.\n",
    "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        \n",
    "        # negate final_state_locations\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        \n",
    "        non_final_states = next_states[non_final_state_locations] \n",
    "        \n",
    "        # shape[0]: length of 0th dimension, meaning how many next_states are available\n",
    "        batch_size = next_states.shape[0]\n",
    "        \n",
    "        #send empty values to device\n",
    "        values = torch.zeros(batch_size).to(QValues.device)\n",
    "        \n",
    "        # infer action values using states, select max action value\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        \n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0de932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Notes for tensor\n",
    "\n",
    "# a = [1, 2, 3, 4]\n",
    "# b = [1, 0, 0, 1]\n",
    "# a = torch.tensor(a)\n",
    "# b = torch.tensor(b).type(torch.bool)\n",
    "# a[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f214aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(np.random.rand(300), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee924b41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MAIN PROGRAM ==============================================================\n",
    "\n",
    "# HYPERPARAMETERS ===========================================================\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "# discount factor for discounted return\n",
    "gamma = 0.99\n",
    "\n",
    "# exploration rate\n",
    "# previously decay rate was too high at 0.001\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.0001\n",
    "\n",
    "# number of episodes before updating the target network\n",
    "target_update = 5\n",
    "\n",
    "# replay memory size\n",
    "# debugged: setting memory size too large caused cuda memory to run out\n",
    "memory_size = 20000\n",
    "\n",
    "# learning rate\n",
    "lr = 0.001\n",
    "\n",
    "num_episodes = 1000\n",
    "\n",
    "# OBJECT INSTANCES ===========================================================\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "em = LunarLanderEnvManager(device)\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy, em.num_actions_available(), device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "target_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "\n",
    "# copy weights and biases\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# evaluation mode tells pytorch that this is not for training but only for inference\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "\n",
    "# MAIN LOOP =================================================================\n",
    "\n",
    "episode_rewards = []\n",
    "step_losses = []\n",
    "episode_losses = []\n",
    "\n",
    "# episode loop\n",
    "for episode in range (num_episodes):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    total_reward = 0\n",
    "    \n",
    "    # timestep loop\n",
    "    for timestep in count():\n",
    "        \n",
    "        # gaining experience\n",
    "        action = agent.select_action(state,policy_net)\n",
    "        reward = em.take_action(action)\n",
    "        total_reward += reward.item()\n",
    "        next_state = em.get_state()\n",
    "        memory.push(Experience(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "    \n",
    "        # learning from experience\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "\n",
    "            # get Q value predicted by the policy network from the current state and action\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            current_q_values = current_q_values.float()\n",
    "\n",
    "            # get the Q value predicted by the target network from the next state\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            next_q_values = next_q_values.float()\n",
    "\n",
    "            # Bellman Optimality Equation\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "            target_q_values = target_q_values.float()\n",
    "\n",
    "            # mse is mean squared error\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "\n",
    "            # set gradients of weights and biases to zero\n",
    "            # to avoid previous gradients from accumulating\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights and biases based on the loss\n",
    "            optimizer.step()\n",
    "            \n",
    "            # store average loss, averaging over batch_size each timestep\n",
    "            step_losses.append(np.mean(loss.item()))\n",
    "            \n",
    "        if em.done or timestep > 2000:\n",
    "            episode_rewards.append(total_reward)\n",
    "            if len(step_losses):\n",
    "                episode_losses.append(np.mean(step_losses))\n",
    "            step_losses.clear()\n",
    "            plot(episode_rewards, 50, agent.current_step, eps_decay, episode_losses)\n",
    "            total_reward = 0\n",
    "            break\n",
    "    \n",
    "    # update target network when episode reaches a multiple of the update period\n",
    "    if episode % target_update == 0:\n",
    "        \n",
    "        # copy the weights and biases of the policy network to the target network\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "em.close()\n",
    "\n",
    "# save model\n",
    "FILE = \"model.pth\"\n",
    "torch.save(policy_net, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1cab7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
