{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import openpyxl\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn import DQN\n",
    "from environment import LunarLanderEnvManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXPERIENCE CLASS\n",
    "\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'next_state', 'reward', 'done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLAY MEMORY CLASS\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPSILON GREEDY STRATEGY CLASS\n",
    "\n",
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENT CLASS\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "\n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=1).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PERFORMANCE PLOTS\n",
    "\n",
    "def plot(rewards_per_episode, moving_average, losses):\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(rewards_per_episode)\n",
    "    plt.plot(moving_average)\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(losses)\n",
    "    plt.pause(0.001)\n",
    "    \n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1).mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSOR PROCESSING FUNCTION\n",
    "\n",
    "def extract_tensors(expriences):\n",
    "    batch = Experience(*zip(*experiences))\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "    t5 = torch.cat(batch.done)\n",
    "    return (t1, t2, t3, t4, t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-VALUE CALCULATOR CLASS\n",
    "\n",
    "class QValues():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_next(target_net, next_states):\n",
    "        return target_net(next_states).max(dim=1)[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_next_ddqn(policy_net, target_net, next_states):\n",
    "        with torch.no_grad():\n",
    "            actions = policy_net(next_states).argmax(dim=1)\n",
    "        return target_net(next_states).gather(dim=1, index=actions.unsqueeze(-1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MAIN PROGRAM\n",
    "\n",
    "# SETTINGS\n",
    "num_episodes = 1000\n",
    "render = False\n",
    "double_dqn = False\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 500\n",
    "memory_size = 50000\n",
    "lr = 0.001\n",
    "\n",
    "# OBJECT INSTANCES\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "em = LunarLanderEnvManager(device)\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy, em.num_actions_available(), device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_net = DQN(em.num_states_available()).to(device)\n",
    "target_net = DQN(em.num_states_available()).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "\n",
    "rewards_per_episode = []\n",
    "losses = []\n",
    "\n",
    "# MAIN LOOP\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    total_reward = 0\n",
    "\n",
    "    # timestep loop\n",
    "    for timestep in count():\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        reward = em.take_action(action)\n",
    "        done = em.if_done()\n",
    "        next_state = em.get_state()\n",
    "        memory.push(Experience(state, action, next_state, reward, done))\n",
    "        state = next_state\n",
    "        total_reward += reward.item()\n",
    "\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states, dones = extract_tensors(experiences)\n",
    "\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            current_q_values = current_q_values.float()\n",
    "\n",
    "            if double_dqn:\n",
    "                next_q_values = QValues.get_next_ddqn(policy_net, target_net, next_states)\n",
    "            else:\n",
    "                next_q_values = QValues.get_next(target_net, next_states)\n",
    "            next_q_values = next_q_values.float()\n",
    "\n",
    "            target_q_values = (next_q_values * gamma) * (1 - dones) + rewards\n",
    "            target_q_values = target_q_values.float()\n",
    "\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if agent.current_step % target_update == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if render:\n",
    "            em.render()\n",
    "\n",
    "        if em.done:\n",
    "            rewards_per_episode.append(total_reward)\n",
    "            moving_average = get_moving_average(50, rewards_per_episode)\n",
    "            plot(rewards_per_episode, moving_average, losses)\n",
    "            print(\"Episode\", len(rewards_per_episode))\n",
    "            print(\"Average:\", moving_average[-1])\n",
    "            print(\"Steps\", timestep)\n",
    "            if is_ipython: display.clear_output(wait=True)\n",
    "            break\n",
    "\n",
    "        if moving_average[-1] >= 200:\n",
    "            date_time = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "            FILE = \"models/model_\" + date_time + \".pth\"\n",
    "            torch.save(policy_net.state_dict(), FILE)\n",
    "            break\n",
    "\n",
    "em.close()\n",
    "\n",
    "# workbook = openpyxl.load_workbook('data.xlsx')\n",
    "# worksheet = workbook['sheet1']\n",
    "\n",
    "# worksheet.append(moving_average.tolist())\n",
    "# worksheet.append(rewards_per_episode)\n",
    "\n",
    "# workbook.save('data.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
